### **第一步：找出所有的面孔**

首先我们把图片灰度化，因为颜色信息对于人脸检测而言没什么用。

我们分析每个像素以及其周围的像素，根据明暗度画一个箭头，箭头的指向代表了像素逐渐变暗的方向，如果我们重复操作每一个像素，最终像素会被箭头取代。这些箭头被称为梯度(gradients)，它们能显示出图像从明亮到黑暗流动的过程。

![image-20200605151955771](E:/Git图库/image-20200605151955771.png)

可以得到

![img](E:/Git图库/20180802144956436.png)

### **第二步：脸部的不同姿势**

面部特征点估计（face landmark estimation）的算法，瓦希德·卡奇米（Vahid Kazemi）和约瑟菲娜·沙利文（Josephine Sullivan）在 2014 年发明的方法。

这一算法的基本思路是找到68个人脸上普遍存在的点（称为特征点， landmark）。

- 下巴轮廓17个点 [0-16]
- 左眉毛5个点 [17-21]
- 右眉毛5个点 [22-26]
- 鼻梁4个点 [27-30]
- 鼻尖5个点 [31-35]
- 左眼6个点 [36-41]
- 右眼6个点 [42-47]
- 外嘴唇12个点 [48-59]
- 内嘴唇8个点 [60-67]

有了这68个点，我们就可以轻松的知道眼睛和嘴巴在哪儿了，后续我们将图片进行旋转，缩放和错切，使得眼睛和嘴巴尽可能的靠近中心

### **第三步：给脸部编码**

我们人类能通过眼睛大小，头发颜色等等信息轻松的分辨不同的两张人脸，可是电脑怎么分辨呢？没错，我们得量化它们，测量出他们的不同，那要怎么做呢？

实际上，对于人脸这些信息很容易分辨，可是对于计算机，这些值没什么价值。实际上最准确的方法是让计算机自己找出他要收集的测量值。深度学习比人类更懂得哪些面部测量值比较重要。

所以，解决方案是训练一个深度卷积神经网络，训练让它为脸部生成128个测量值。

每次训练要观察三个不同的脸部图像：

加载一张已知的人的面部训练图像
加载同一个人的另一张照片
加载另外一个人的照片
然后，算法查看它自己为这三个图片生成的测量值。再然后，稍微调整神经网络，以确保第一张和第二张生成的测量值接近，而第二张和第三张生成的测量值略有不同。

我们要不断的调整样本，重复以上步骤百万次，这确实是个巨大的挑战，但是一旦训练完成，它能攻轻松的找出人脸。

庆幸的是 OpenFace 上面的大神已经做完了这些，并且他们发布了几个训练过可以直接使用的网络，我们可以不用部署复杂的机器学习，开箱即用，感谢开源精神。
![image-20200605152207892](E:/Git图库/image-20200605152207892.png)

### **第四步：从编码中找出人的名字**

最后一步实际上是最简单的一步，我们需要做的是找到数据库中与我们的测试图像的测量值最接近的那个人。如何做呢，我们利用一些现成的数学公式，计算两个128D数值的欧氏距离。

![img](E:/Git图库/20180802145258897.png)







### 总结：

1. 使用找出图片中所有人脸的位置。
2. 计算出人脸的68个特征点并适当的调整人脸位置，对齐人脸。
3. 把上一步得到的面部图像放入神经网络，得到128个特征测量值，并保存它们。
4. 与我们以前保存过的测量值一并计算欧氏距离，得到欧氏距离值，比较数值大小，即可得到是否同一个人。